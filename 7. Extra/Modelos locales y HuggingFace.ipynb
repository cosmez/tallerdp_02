{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyP876/aj66ts1l9QxrRDPY9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# LLM Locales, HuggingFace y BERT"],"metadata":{"id":"ujTWnLgVt5WU"}},{"cell_type":"markdown","source":["# BERT\n","\n","Los modelos de lenguaje fueron creados para todo tipo de tareas NLP, de hecho desde su concepción en la tesis de [Attention is all you need](https://arxiv.org/abs/1706.03762) los modelos de lenguaje fueron creados como un modelos no generativos.\n","\n","Hay modelos de lenguaje para todo tipo de tareas como:\n","\n","1. Reconocimiento de Entidades\n","2. Análisis de Sentimiento\n","3. Preguntas y Respuestas\n","4. Traducción\n","5. Resumir\n","6. Generar Embeddings\n","\n","[BERT](https://huggingface.co/blog/bert-101) a pesar de ser de los primeros modelos de lenguaje (transformer), sus variantes siguen siendo las opciones más utilizadas para cualquier tarea que no sea generación de texto, donde principalmente usan arquitecturas basadas en GPT.\n","\n","Muchos de los experimentos en los que hemos trabajado usan algún modelo de lenguaje no generativo para complementar al modelo generativo. Son modelos especializados, más pequeños, eficientes y que fácilmente los podemos ejecutar on-prem."],"metadata":{"id":"YqYReOC1k-Us"}},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","from transformers import pipeline\n","import pandas as pd\n","import torch\n"],"metadata":{"id":"iVVHWwnEbOSE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Hugging Face\n","\n","Hugging Face es otra herramienta indispensable para trabajar con modelos de lenguaje locales ya sea generativos o no generativos.\n","\n","En estos ejercicios voy a explicarles cómo me apoyo de la plataforma para llevar a cabo nuestros experimentos.\n","\n","Lo primero que tenemos que hacer es [crear una cuenta](https://huggingface.co/join) y [generar un token](https://huggingface.co/settings/tokens) para poder descargar los modelos desde nuestro notebook.\n","\n","Palabras Clave:\n","\n","* Transformers\n","* Inference\n","* Pipeline\n","* Task\n"],"metadata":{"id":"LedWalFFc3io"}},{"cell_type":"code","source":["notebook_login()"],"metadata":{"id":"LQL9dEbxc2km"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generación de Texto\n","\n","Es con lo que hemos trabajado hasta ahorita, lo que hizo popular a los modelos de lenguaje. Son los modelos más pesados y desarrollar con ellos de manera local es casi imposible para el hardware de consumo general."],"metadata":{"id":"3i-asrpuetm0"}},{"cell_type":"markdown","source":["## Tasks"],"metadata":{"id":"Ckp3Pv2Ut1DQ"}},{"cell_type":"code","source":["gpt_generator = pipeline(\"text-generation\", model = \"meta-llama/Llama-3.2-1B\")\n","print(gpt_generator(\"Mi nombre es Juan y tengo \", max_length=20))"],"metadata":{"id":"4BqGnpMS6w_K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Reconocimiento de Entidades (NER)\n","\n","Identificación de personas, organizaciones, lugares, etc."],"metadata":{"id":"07Z1fHCed5Xx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"PDwsiiCy4HMX"},"outputs":[],"source":["classifier = pipeline(\"ner\", model = \"iiiorg/piiranha-v1-detect-personal-information\")\n","classifier(\"Mi nombre es Juan y vivo en Sinaloa.\")"]},{"cell_type":"markdown","source":["### Análisis de Sentimiento\n","\n","Modelos especializados en detectar emociones y opiniones."],"metadata":{"id":"fnj3M5EefFzF"}},{"cell_type":"code","source":["sentiment_task = pipeline(\"sentiment-analysis\", model=\"clapAI/modernBERT-base-multilingual-sentiment\", tokenizer=\"clapAI/modernBERT-base-multilingual-sentiment\")\n","sentiment_task(\"no me gusta\")"],"metadata":{"id":"yp_E1uYqr5yI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Mask\n","\n","El modelado de lenguaje enmascarado es la tarea de ocultar algunas de las palabras en una oración y predecir qué palabras deberían reemplazar esas máscaras.\n","\n","Este tipo de modelos nunca los he usado, creo que su tarea es muy específica y su uso más práctico es muy parecido a un modelo generativo, sin ser autorregresivo.\n","\n","Para este ejemplo voy a aprovechar y voy a usar una API un poquito de más bajo nivel para explicar otros conceptos."],"metadata":{"id":"HggGadwCfkpY"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForMaskedLM\n","\n","model_id = \"answerdotai/ModernBERT-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = AutoModelForMaskedLM.from_pretrained(model_id)\n","\n","text = \"The capital of France is [MASK].\"\n","inputs = tokenizer(text, return_tensors=\"pt\")\n","outputs = model(**inputs)\n","\n","# To get predictions for the mask:\n","masked_index = inputs[\"input_ids\"][0].tolist().index(tokenizer.mask_token_id)\n","predicted_token_id = outputs.logits[0, masked_index].argmax(axis=-1)\n","predicted_token = tokenizer.decode(predicted_token_id)\n","print(\"Token ID Predecido:\", predicted_token_id)\n","print(\"Token Predecido:\", predicted_token)\n","print(\"Tokens Totales:\", len(outputs.logits[0, masked_index]))"],"metadata":{"id":"dm_1VG1xr9hT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Salida y Top_K\n","\n","En este caso el modelo de lenguaje nos regresa las probabilidades del token que se encuentra en masked_index, que es el índice de nuestra máscara. **[MASK]** es un token especial para este task de BERT, así como los tokens de tools, pensamiento o de formato de chat.\n","\n","Con esta salida podemos hacer una selección usando algún algoritmo de selección, como los que ya conocemos como temperatura, top_k o top_p.\n","\n"],"metadata":{"id":"b0Q1KF26g9G-"}},{"cell_type":"code","source":["# Obtener las probabilidades de los tokens en la posición de la máscara\n","mask_logits = outputs.logits[0, masked_index]\n","\n","\n","# Convierte logits a probabilidades usando softmax\n","probabilities = torch.softmax(mask_logits, dim=-1)\n","top_k = 5\n","\n","print(f\"\\nTop {top_k} predicciones con probabilidad [MASK]:\")\n","top_k_prob, top_k_indices = probabilities.topk(top_k)\n","\n","for i in range(5):\n","    token = tokenizer.decode(top_k_indices[i])\n","    percentage = top_k_prob[i].item() * 100\n","    print(f\"Token: {token}, Percentage: {percentage:.2f}%\")\n"],"metadata":{"id":"wp02eRWEheQt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Este modelo nos da como salida la probabilidad de todos los tokens de su vocabulario, es responsabilidad nuestra elegir cuál token queremos.\n","\n","Los modelos generativos como Gemini son muy parecidos a esto, pero solo pueden predecir basándose en el texto que se encuentra antes del último token."],"metadata":{"id":"JrxW6E22ibDm"}},{"cell_type":"code","source":["# Saca una lista de todos los tokens y todas sus probabilidades\n","all_token_ids = torch.arange(probabilities.size(0))\n","all_tokens = [tokenizer.decode(token_id) for token_id in all_token_ids]\n","all_probabilities = probabilities.tolist()\n","\n","data = []\n","for token, prob in zip(all_tokens, all_probabilities):\n","    data.append({'Token': token, 'probabilidad': prob, '%': f'{(prob * 100):.3f}%'})\n","\n","df_all_tokens = pd.DataFrame(data)\n","\n","df_all_tokens_sorted = df_all_tokens.sort_values(by='%', ascending=False).reset_index(drop=True)\n","\n","print(\"\\nDataframe con todos los tokens, probabilidades, y porcentajes:\")\n","df_all_tokens_sorted\n"],"metadata":{"id":"2ssxLUyQi_IT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Ejercicio\n","\n","¿Qué hacen los modelos que responden a preguntas, si no son generativos?\n","\n","Modifica el siguiente código con un modelo y el texto necesario."],"metadata":{"id":"PMp6R7dJrtZ5"}},{"cell_type":"code","source":["qa_model = pipeline(\"question-answering\", model=)\n","question = \"\"\n","context = \"Culiacán is the largest city in Sinaloa with a population of approximately 1 million people in its metropolitan area. Culiacán is the capital city of Sinaloa state in northwestern Mexico. \"\n","qa_model(question = question, context = context)"],"metadata":{"id":"rBbCOEKpsTRu"},"execution_count":null,"outputs":[]}]}